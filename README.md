<img src="./assets/laas_logo.png" alt="Logo Laas" width="200"/>
<br></br>

# Intent-Based Network Agent (Clab Agent)


Artificial Intelligence for Intent-Based Network Modeling with ContainerLab

*Intelligent agent system developed at LAAS-CNRS to automate the generation and deployment of network topologies from natural language.*

### ğŸ‘¨â€ğŸ“ Auteur
Nathan dos Reis Ruba \
Student-Engineer in Automation-Electronics Embedded Systems \
<b>SpÃ©cialisation :</b> Cybersecurity (M2 TLS-SEC)

ğŸ“ INSA Toulouse - Institut National des Sciences AppliquÃ©es

âœ‰ï¸ Email : nathan.ruba2003@gmail.com \
ğŸ”— LinkedIn : [Nathan dos Reis Ruba](https://www.linkedin.com/in/nathan-dos-reis-ruba-1062b7145/)


## ğŸ“ Contexte AcadÃ©mique
This project was developed as part of a 3-month internship at LAAS-CNRS (Laboratory of Analysis and Architecture of Systems) in Toulouse, France.

### ğŸ›ï¸ Instituition
- Engineering school: INSA Toulouse
- Laboratory: LAAS-CNRS, Toulouse
- Team: Services and Architectures for Advanced Networks (SARA)
- AnnÃ©e: 2025

## ğŸ“¦ Installation

### 1. System Prerequisites
- <b>Operating System:</b> Linux Distro (Ubuntu recommended)


```bash
# ContainerLab
sudo bash -c "$(curl -sL https://get.containerlab.dev)"

# Ollama for local LLMs
curl -fsSL https://ollama.com/install.sh | sh

# Required models
ollama pull qwen3:latest
ollama pull qwen2.5:3b  
ollama pull qwen2.5-coder:3b
```

### 2. Python Environment
```bash
# Clone the project
git clone https://github.com/Ruba008/clab_agent.git
cd clab_agent

# Virtual environment
conda create -n clab_agent python=3.11.13
conda activate clab_agent

# Dependencies
pip install -r requirements.txt
```

### 3. Google Cloud Configuration
```bash
# Authentication for Dialogflow
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/credentials.json"
```

### 4. Import DialogFlow Project
Go to `src/intent_classifier.zip` and import the files to [DialogFlow](https://dialogflow.cloud.google.com/).

### 5. IMPORTANT: configure the DialogFlow
Open `src/tools/models.py` and set:

```python
 # Google Cloud project configuration
    PROJECT_ID = "" # Dialogflow project ID
    SESSION_ID = "" # Static session for consistent context
    LANG = "en" # Language code for input processing
```


## ğŸ“š Technical Documentation

### Scrapy

For scrapy the ContainerLab documentation you need to access `src` and execute `scrapy runspider ./tools/scrapy_documentation.py`.

### Project Structure
```
clab_agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # Main entry point
â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py     # Orchestration module
â”‚   â”‚   â”œâ”€â”€ researcher.py       # Research module
â”‚   â”‚   â”œâ”€â”€ runner.py          # Execution module
â”‚   â”‚   â”œâ”€â”€ schema.py          # Pydantic models
â”‚   â”‚   â””â”€â”€ instructions/      # LLM prompts
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ db.py              # ChromaDB interface
â”‚       â”œâ”€â”€ models.py          # LLM management
â”‚       â””â”€â”€ scrapy_documentation.py
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                 # Documentation
```
### Project Diagram
<br></br>
<img src="./assets/diagramme.svg" alt="Diagramme"/>
<br></br>
<br></br>
- <b>Orchestrator :</b> module responsible for interpreting, explaining, identifying and planning the user's objective.
- <b>Research :</b> module responsible for consuming up-to-date data from the database, ranking the best information and summarizing it.
- <b>Runner :</b> module responsible for coding, checking runtime dependencies and performing the deployment.

### Technologies Used
#### Artificial Intelligence

- ğŸ¦™ Ollama : Local LLM server (Qwen 3, Qwen 2.5, Qwen 2.5-Coder)
- ğŸŒ Google Dialogflow : Intent classification
- ğŸ”— LangChain : LLM chain orchestration
- ğŸ“Š Sentence Transformers : Cross-domain semantic encoders

#### Base de DonnÃ©es & Recherche

- ğŸ¨ ChromaDB : Vector database for semantic search
- ğŸ¤— Hugging Face : Embedding models (all-MiniLM-L6-v2)
- ğŸ“„ Scrapy : Document extraction framework

#### Orchestration & Graphes

- ğŸ”„ LangGraph : State graphs for complex workflows
- ğŸ“ StateGraph : Management of transitions and dependencies
- ğŸ¯ Pydantic : Schema validation and data models

#### Conteneurisation & RÃ©seau

- ğŸ³ Docker API : Programmatic container management
- ğŸ§ª ContainerLab : Network topology deployment

#### Interface & Visualisation

- ğŸ¨ Rich Console : Advanced terminal user interface
- ğŸ“Š Web Visualization : Interactive topology graphs (clab graph)
- âš¡ Live Updates : Real-time feedback of the AI process

## RÃ©sultats

<b>Validated Use Cases:</b>
"Create a network star topology connected with 10 Mbps maximum"

### YAML Output
```yaml
name: star-bridge-mtu
mgmt:
  network: clab
  ipv4-subnet: 172.20.20.0/24
topology:
  nodes:
    central:
      kind: linux
      image: alpine:latest
      mgmt-ipv4: 172.20.20.11
    node1:
      kind: linux
      image: alpine:latest
      mgmt-ipv4: 172.20.20.12
    node2:
      kind: linux
      image: alpine:latest
      mgmt-ipv4: 172.20.20.13
    node3:
      kind: linux
      image: alpine:latest
      mgmt-ipv4: 172.20.20.14
  links:
    - endpoints: ["central:eth1", "node1:eth1"]
      mtu: 1500
    - endpoints: ["central:eth2", "node2:eth1"]
      mtu: 1500
    - endpoints: ["central:eth3", "node3:eth1"]
      mtu: 1500
```
### Clab Graph
<img src="./assets/clab_graph.jpeg" alt=""/>

### Clab Inspect
| Name | Kind/Image | State | IPv4/6 Address |
| ----------- | ----------- | ----------- | ----------- |
| clab_star-bridge-mtu-central | linux alpine:latest | created | N/A |
| clab_star-bridge-mtu-node1 | linux alpine:latest | created | N/A |
| clab_star-bridge-mtu-node2 | linux alpine:latest | created | N/A |
| clab_star-bridge-mtu-node3 | linux alpine:latest | created | N/A |

## ğŸ™ Acknowledgements

- <b>LAAS-CNRS : </b> For the welcome and research resources.
- <b>Supervisors : </b> Pascal Berthou and Slim Abdellatif.